{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ## Question 1\n",
        "# **What is the difference between K-Means and Hierarchical Clustering? Provide a use case for each.**\n",
        "#\n",
        "# **Answer:**\n",
        "# - **K-Means:** A partitioning method that assigns data to k clusters by minimizing within-cluster variance. It requires specifying k beforehand, is efficient on large datasets, and assumes roughly spherical clusters of similar size.\n",
        "#   - *Use case:* Customer segmentation with a known/suspected number of groups and many data points (e.g., segmenting customers by purchase frequency and average order value).\n",
        "# - **Hierarchical Clustering:** Builds a dendrogram (agglomerative or divisive). Agglomerative starts with each point as its own cluster and merges iteratively. No need to pre-specify the number of clusters (you can cut the dendrogram), but it is more computationally expensive.\n",
        "#   - *Use case:* Exploratory analysis of gene expression data where hierarchy and relationships between clusters are important.\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Question 2\n",
        "# **Explain the purpose of the Silhouette Score in evaluating clustering algorithms.**\n",
        "#\n",
        "# **Answer:**\n",
        "# - The Silhouette Score measures how similar an object is to its own cluster compared to other clusters. It ranges from -1 to 1.\n",
        "#   - Values near +1 indicate the sample is well matched to its cluster and poorly matched to neighboring clusters.\n",
        "#   - Values near 0 indicate overlapping clusters.\n",
        "#   - Negative values indicate potential misclassification.\n",
        "# - It is useful to compare clustering quality across different algorithms or different numbers of clusters.\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Question 3\n",
        "# **What are the core parameters of DBSCAN, and how do they influence the clustering process?**\n",
        "#\n",
        "# **Answer:**\n",
        "# - **eps (epsilon):** The neighborhood radius. Points within this distance are considered neighbors.\n",
        "# - **min_samples:** Minimum number of points (including the point itself) required to form a dense region (core point).\n",
        "# - Influence:\n",
        "#   - Increasing **eps** yields larger neighborhoods → fewer clusters, less noise.\n",
        "#   - Increasing **min_samples** makes it harder to form clusters → more points labeled as noise.\n",
        "# - DBSCAN can find arbitrarily shaped clusters and handles noise, but struggles with varying densities.\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Question 4\n",
        "# **Why is feature scaling important when applying clustering algorithms like K-Means and DBSCAN?**\n",
        "#\n",
        "# **Answer:**\n",
        "# - Many clustering algorithms use distance metrics (e.g., Euclidean). Features with larger scales dominate distance computations and bias cluster assignments.\n",
        "# - Scaling (StandardScaler, MinMaxScaler) ensures each feature contributes proportionally.\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Question 5\n",
        "# **What is the Elbow Method in K-Means clustering and how does it help determine the optimal number of clusters?**\n",
        "#\n",
        "# **Answer:**\n",
        "# - The Elbow Method computes the within-cluster sum of squares (inertia) for a range of k values and plots inertia vs. k.\n",
        "# - Initially, adding clusters reduces inertia significantly; after a point the marginal gain drops — the \"elbow\" suggests a reasonable k.\n",
        "\n",
        "# %% [markdown]\n",
        "# ---\n",
        "# ## Dataset instructions\n",
        "# Use `make_blobs`, `make_moons`, and `sklearn.datasets.load_wine()` as specified in subsequent questions.\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Question 6\n",
        "# **Generate synthetic data using `make_blobs(n_samples=300, centers=4)`, apply KMeans clustering, and visualize the results with cluster centers.**\n",
        "\n",
        "# %%\n",
        "# Question 6 - Code\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# generate data\n",
        "X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=1.0, random_state=42)\n",
        "\n",
        "# apply KMeans\n",
        "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
        "labels = kmeans.fit_predict(X)\n",
        "centers = kmeans.cluster_centers_\n",
        "\n",
        "# plot\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(X[:,0], X[:,1], c=labels, s=30, cmap='tab10', alpha=0.7)\n",
        "plt.scatter(centers[:,0], centers[:,1], c='black', s=200, marker='X', label='Centers')\n",
        "plt.title('Q6: KMeans on make_blobs (4 centers)')\n",
        "plt.legend()\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()\n",
        "\n",
        "print('Cluster centers:\\n', centers)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Question 7\n",
        "# **Load the Wine dataset, apply `StandardScaler`, and then train a DBSCAN model. Print the number of clusters found (excluding noise).**\n",
        "\n",
        "# %%\n",
        "# Question 7 - Code\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "wine = load_wine(as_frame=True)\n",
        "X_wine = wine.data\n",
        "\n",
        "# scale\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_wine)\n",
        "\n",
        "# DBSCAN\n",
        "db = DBSCAN(eps=0.9, min_samples=5)  # eps chosen reasonably for scaled data\n",
        "labels_db = db.fit_predict(X_scaled)\n",
        "\n",
        "# number of clusters (excluding noise label -1)\n",
        "n_clusters = len(set(labels_db)) - (1 if -1 in labels_db else 0)\n",
        "print(f'Number of clusters found (excluding noise): {n_clusters}')\n",
        "\n",
        "# print counts per label\n",
        "unique, counts = np.unique(labels_db, return_counts=True)\n",
        "print('Label counts:')\n",
        "for lab, cnt in zip(unique, counts):\n",
        "    print(f'  Label {lab}: {cnt}')\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Question 8\n",
        "# **Generate moon-shaped synthetic data using `make_moons(n_samples=200, noise=0.1)`, apply DBSCAN, and highlight the outliers in the plot.**\n",
        "\n",
        "# %%\n",
        "# Question 8 - Code\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "X_moons, y_moons = make_moons(n_samples=200, noise=0.1, random_state=42)\n",
        "# DBSCAN with euclidean distance: tune eps\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# quick heuristic: compute k-distance plot to pick eps (not displayed here, but we pick a reasonable value)\n",
        "db_moons = DBSCAN(eps=0.2, min_samples=5)\n",
        "labels_moons = db_moons.fit_predict(X_moons)\n",
        "\n",
        "# mask outliers\n",
        "outliers_mask = labels_moons == -1\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(X_moons[~outliers_mask,0], X_moons[~outliers_mask,1], c=labels_moons[~outliers_mask], s=40, cmap='tab10', alpha=0.7)\n",
        "plt.scatter(X_moons[outliers_mask,0], X_moons[outliers_mask,1], c='red', s=60, marker='x', label='Outliers')\n",
        "plt.title('Q8: DBSCAN on make_moons — Outliers highlighted')\n",
        "plt.legend()\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()\n",
        "\n",
        "print('Unique labels (including -1 for noise):', set(labels_moons))\n",
        "print('Number of outliers detected:', outliers_mask.sum())\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Question 9\n",
        "# **Load the Wine dataset, reduce it to 2D using PCA, then apply Agglomerative Clustering and visualize the result in 2D with a scatter plot.**\n",
        "\n",
        "# %%\n",
        "# Question 9 - Code\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# reduce to 2D\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Agglomerative clustering\n",
        "agg = AgglomerativeClustering(n_clusters=3)  # choose 3 for demonstration\n",
        "agg_labels = agg.fit_predict(X_pca)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(X_pca[:,0], X_pca[:,1], c=agg_labels, s=40, cmap='tab10', alpha=0.8)\n",
        "plt.title('Q9: Agglomerative Clustering on Wine (PCA -> 2D)')\n",
        "plt.xlabel('PCA 1')\n",
        "plt.ylabel('PCA 2')\n",
        "plt.show()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Question 10\n",
        "# **You are working as a data analyst at an e-commerce company. The marketing team wants to segment customers based on their purchasing behavior to run targeted promotions. The dataset contains customer demographics and their product purchase history across categories.**\n",
        "#\n",
        "# **Describe your real-world data science workflow using clustering:**\n",
        "# - Which clustering algorithm(s) would you use and why?\n",
        "# - How would you preprocess the data (missing values, scaling)?\n",
        "# - How would you determine the number of clusters?\n",
        "# - How would the marketing team benefit from your clustering analysis?\n",
        "#\n",
        "# *(Include Python code and output in the code box below.)*\n",
        "\n",
        "# %%\n",
        "# Question 10 - Answer + Example Code\n",
        "# --------------------------------------------------\n",
        "# 1) Algorithm choice (short):\n",
        "# - Use KMeans for a baseline if data roughly convex and clusters expected; it's fast and interpretable.\n",
        "# - Use DBSCAN if expecting noise or arbitrary-shaped clusters and want outlier detection.\n",
        "# - Use Gaussian Mixture Models (GMM) if soft cluster memberships are desired.\n",
        "# - Use hierarchical clustering for exploratory hierarchy.\n",
        "#\n",
        "# 2) Preprocessing:\n",
        "# - Handle missing values: impute numeric features (median) and categorical features (mode) or use model-based imputation.\n",
        "# - Encode categorical variables: OneHotEncoder for nominal, OrdinalEncoder if ordinal.\n",
        "# - Scale numeric features: StandardScaler or MinMaxScaler depending on algorithm.\n",
        "# - Feature engineering: create RFM features (Recency, Frequency, Monetary), CLTV estimates, or normalized category spend ratios.\n",
        "#\n",
        "# 3) Determine number of clusters:\n",
        "# - Use Elbow Method, Silhouette Score, and domain knowledge. Also consider stability testing (bootstrap) and business constraints.\n",
        "#\n",
        "# 4) Business benefit:\n",
        "# - Personalized promotions, targeted email campaigns, product recommendations, tailored discounts, and lifecycle marketing.\n",
        "#\n",
        "# Example pipeline (synthetic demonstration):\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Synthetic demo dataset\n",
        "np.random.seed(42)\n",
        "N = 300\n",
        "customer_demo = pd.DataFrame({\n",
        "    'age': np.random.randint(18,70,size=N),\n",
        "    'gender': np.random.choice(['M','F'], size=N, p=[0.48,0.52]),\n",
        "    'annual_income': np.random.normal(50000,15000,size=N).astype(int),\n",
        "})\n",
        "\n",
        "# Generate RFM-like features\n",
        "purchases = np.random.poisson(lam=5, size=N)\n",
        "avg_order_value = np.abs(np.random.normal(100,50,size=N))\n",
        "recency_days = np.random.randint(1,365,size=N)\n",
        "\n",
        "customer_demo['purchases'] = purchases\n",
        "customer_demo['avg_order_value'] = avg_order_value\n",
        "customer_demo['recency_days'] = recency_days\n",
        "\n",
        "# pipeline: encode gender, scale numerical\n",
        "num_features = ['age','annual_income','purchases','avg_order_value','recency_days']\n",
        "cat_features = ['gender']\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), num_features),\n",
        "        ('cat', OneHotEncoder(drop='if_binary'), cat_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('prep', preprocessor),\n",
        "    ('kmeans', KMeans(n_clusters=4, random_state=42))\n",
        "])\n",
        "\n",
        "pipe.fit(customer_demo)\n",
        "labels_demo = pipe.named_steps['kmeans'].labels_\n",
        "customer_demo['segment'] = labels_demo\n",
        "\n",
        "# Brief visualisation of segments over two principal components\n",
        "from sklearn.decomposition import PCA\n",
        "X_prep = pipe.named_steps['prep'].transform(customer_demo.drop(columns=['segment']))\n",
        "X_2d = PCA(n_components=2, random_state=42).fit_transform(X_prep)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(X_2d[:,0], X_2d[:,1], c=labels_demo, s=40, cmap='tab10', alpha=0.8)\n",
        "plt.title('Q10: Example Customer Segments (synthetic)')\n",
        "plt.xlabel('PCA 1')\n",
        "plt.ylabel('PCA 2')\n",
        "plt.show()\n",
        "\n",
        "# Show top-level segment summary\n",
        "summary = customer_demo.groupby('segment').agg({\n",
        "    'purchases':'mean',\n",
        "    'avg_order_value':'mean',\n",
        "    'recency_days':'mean',\n",
        "    'annual_income':'mean',\n",
        "    'age':'mean',\n",
        "    'gender':lambda x: x.value_counts().index[0]\n",
        "}).round(2)\n",
        "\n",
        "print('\\nSegment summary:')\n",
        "print(summary)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# End of assignment notebook\n"
      ],
      "metadata": {
        "id": "8W22PTSVf6yd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}